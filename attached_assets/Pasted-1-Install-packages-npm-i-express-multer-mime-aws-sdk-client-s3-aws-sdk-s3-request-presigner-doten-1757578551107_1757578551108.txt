1) Install packages
npm i express multer mime @aws-sdk/client-s3 @aws-sdk/s3-request-presigner dotenv

2) Environment variables

Create a .env with your bucket credentials (from the Replit “Create new bucket” modal). Replit App Storage is S3-compatible, so it works with the AWS SDK.

# From Replit App Storage details
S3_ENDPOINT=https://<your-endpoint>          # e.g., https://us-east-1.replitstorage.com
S3_REGION=us-east-1                           # or whatever region Replit shows
S3_ACCESS_KEY_ID=xxxxxxxxxxxxxxxx
S3_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxx
S3_BUCKET=action-ladder-uploads               # your bucket name

# App
PORT=3000
MAX_FILE_MB=50
ALLOWED_MIME=image/png,image/jpeg,video/mp4,application/pdf

3) s3 client helper (s3.js)
import { S3Client } from "@aws-sdk/client-s3";

export const s3 = new S3Client({
  region: process.env.S3_REGION,
  endpoint: process.env.S3_ENDPOINT,     // S3-compatible endpoint from Replit
  forcePathStyle: true,                   // important for custom endpoints
  credentials: {
    accessKeyId: process.env.S3_ACCESS_KEY_ID,
    secretAccessKey: process.env.S3_SECRET_ACCESS_KEY,
  },
});

4) Express server with secure upload flow (server.js)

This uses presigned URLs so files go directly from browser → bucket (no heavy server memory), with server-side validation of size/type.

import "dotenv/config";
import express from "express";
import crypto from "crypto";
import mime from "mime";
import { PutObjectCommand, GetObjectCommand, ListObjectsV2Command, DeleteObjectCommand } from "@aws-sdk/client-s3";
import { getSignedUrl } from "@aws-sdk/s3-request-presigner";
import { s3 } from "./s3.js";

const app = express();
app.use(express.json());

const BUCKET = process.env.S3_BUCKET;
const MAX_FILE_BYTES = Number(process.env.MAX_FILE_MB || 50) * 1024 * 1024;
const ALLOWED = new Set((process.env.ALLOWED_MIME || "").split(",").filter(Boolean));

/**
 * Create a presigned URL for client-side upload
 * Body: { mime: "image/png", folder?: "players/123" }
 * Returns: { url, fields? (unused), key }
 */
app.post("/api/storage/sign-upload", async (req, res) => {
  try {
    const { mime: mimeType, folder = "uploads" } = req.body || {};
    if (!mimeType || (ALLOWED.size && !ALLOWED.has(mimeType))) {
      return res.status(400).json({ error: "Unsupported file type" });
    }

    // Unique object key: folder/yyyy/mm/dd/random.ext
    const ext = mime.getExtension(mimeType) || "bin";
    const stamp = new Date();
    const key = `${folder}/${stamp.getFullYear()}/${String(stamp.getMonth()+1).padStart(2,"0")}/${String(stamp.getDate()).padStart(2,"0")}/${crypto.randomUUID()}.${ext}`;

    const cmd = new PutObjectCommand({
      Bucket: BUCKET,
      Key: key,
      ContentType: mimeType,
      // Private by default (recommended). Use signed GETs to serve.
      ACL: "private",
    });

    // short expiry minimizes leaked links
    const url = await getSignedUrl(s3, cmd, { expiresIn: 60 }); // 60s
    res.json({ url, key, expiresIn: 60, maxBytes: MAX_FILE_BYTES });
  } catch (e) {
    console.error(e);
    res.status(500).json({ error: "Failed to sign upload" });
  }
});

/**
 * Create a presigned GET URL to show/download a private object
 * Query: ?key=<objectKey>&download=1
 */
app.get("/api/storage/sign-download", async (req, res) => {
  try {
    const { key, download } = req.query;
    if (!key) return res.status(400).json({ error: "Missing key" });

    const cmd = new GetObjectCommand({
      Bucket: BUCKET,
      Key: String(key),
      ResponseContentDisposition: download ? "attachment" : undefined,
    });

    const url = await getSignedUrl(s3, cmd, { expiresIn: 60 }); // 60s
    res.json({ url, expiresIn: 60 });
  } catch (e) {
    console.error(e);
    res.status(500).json({ error: "Failed to sign download" });
  }
});

/**
 * List files under a prefix (e.g., players/123)
 * Query: ?prefix=players/123/
 */
app.get("/api/storage/list", async (req, res) => {
  try {
    const prefix = String(req.query.prefix || "");
    const resp = await s3.send(new ListObjectsV2Command({ Bucket: BUCKET, Prefix: prefix, MaxKeys: 100 }));
    const items = (resp.Contents || []).map(o => ({ key: o.Key, size: o.Size, lastModified: o.LastModified }));
    res.json({ items });
  } catch (e) {
    console.error(e);
    res.status(500).json({ error: "Failed to list" });
  }
});

/**
 * Delete a file (server-side permission check recommended)
 * Body: { key: "uploads/..." }
 */
app.delete("/api/storage/object", async (req, res) => {
  try {
    const { key } = req.body || {};
    if (!key) return res.status(400).json({ error: "Missing key" });
    await s3.send(new DeleteObjectCommand({ Bucket: BUCKET, Key: key }));
    res.json({ ok: true });
  } catch (e) {
    console.error(e);
    res.status(500).json({ error: "Failed to delete" });
  }
});

app.listen(process.env.PORT || 3000, () =>
  console.log(`Storage API running on :${process.env.PORT || 3000}`)
);

5) Frontend example (vanilla JS or React)

Direct upload with the presigned PUT URL:

<input id="file" type="file" />
<button id="upload">Upload</button>
<script>
  const uploadBtn = document.getElementById("upload");
  uploadBtn.onclick = async () => {
    const f = document.getElementById("file").files[0];
    if (!f) return alert("Pick a file");

    // ask backend for one-time upload URL
    const sign = await fetch("/api/storage/sign-upload", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ mime: f.type, folder: "players/123" }),
    }).then(r => r.json());

    if (sign.error) return alert(sign.error);
    if (f.size > sign.maxBytes) return alert("File too large");

    // put directly to bucket
    const put = await fetch(sign.url, {
      method: "PUT",
      headers: { "Content-Type": f.type },
      body: f,
    });

    if (!put.ok) return alert("Upload failed");
    alert("Uploaded: " + sign.key);

    // get a temporary viewing URL
    const view = await fetch(`/api/storage/sign-download?key=${encodeURIComponent(sign.key)}`).then(r => r.json());
    console.log("View URL:", view.url);
  };
</script>

6) Recommended folder strategy (Action Ladder)

players/{playerId}/clips/{yyyy}/{mm}/{dd}/uuid.mp4

players/{playerId}/receipts/{yyyy}/{mm}/{dd}/uuid.pdf

halls/{hallId}/posters/{yyyy}/{mm}/{dd}/uuid.png

reports/{yyyy}/{mm}/{dd}/payouts-{runId}.pdf

7) Security & hygiene

Keep objects private and serve with short-lived signed GET URLs.

Validate MIME & size server-side (already shown).

Namespace paths by user/hall to avoid collisions.

If you later add public assets (logos, posters), you can store those in a separate public/ prefix and serve via CDN if Replit exposes one.

If you want, I can package this as a tiny Replit project (ready to run) or extend it with:

Admin page to browse/delete bucket files

Automatic PDF receipt uploads after Stripe payment

Video size checks + thumbnail extraction pipeline